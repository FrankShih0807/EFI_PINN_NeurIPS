import torch
import torch.nn as nn
import torch.distributions as dist

# Define NetF which uses parameters generated by NetG
class NetF(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(NetF, self).__init__()
        # Define sizes
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim

    def forward(self, x, theta):
        # Extract weights and biases for each layer from theta
        w1 = theta['w1']
        b1 = theta['b1']
        w2 = theta['w2']
        b2 = theta['b2']

        # First hidden layer (ReLU activation)
        h1 = torch.relu(torch.matmul(x, w1) + b1)
        # Output layer (linear activation for regression, can be changed)
        out = torch.matmul(h1, w2) + b2
        
        return out

# Define NetG which generates the parameters for NetF
class NetG(nn.Module):
    def __init__(self, param_input_dim, input_dim, hidden_dim, output_dim):
        super(NetG, self).__init__()
        # Define a network to generate parameters for NetF
        self.param_generator = nn.Sequential(
            nn.Linear(param_input_dim, 128),  # Hidden layers to generate parameters
            nn.ReLU(),
            nn.Linear(128, (input_dim * hidden_dim) + hidden_dim + (hidden_dim * output_dim) + output_dim)
        )
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim    

    def forward(self, xyz):
        # Generate all parameters as a single vector
        param_vector = self.param_generator(xyz)

        # Extract parameters for NetF (reshape them into the correct sizes)
        w1 = param_vector[:, :self.input_dim * self.hidden_dim].view(self.input_dim, self.hidden_dim)
        b1 = param_vector[:, self.input_dim * self.hidden_dim : self.input_dim * self.hidden_dim + self.hidden_dim]
        w2 = param_vector[:, self.input_dim * self.hidden_dim + self.hidden_dim : -self.output_dim].view(self.hidden_dim, self.output_dim)
        b2 = param_vector[:, -self.output_dim:]

        return {'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2}

class EFI_Net(nn.Module):
    def __init__(self, input_dim=1, output_dim=1, hidden_dim=15, activation=nn.ReLU, prior_sd=0.1, sparse_sd=0.01 , sparsity=0.5):
        super(EFI_Net, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.hidden_dim = hidden_dim
        self.activation_fn = activation()
        
        self.sparsity = sparsity
        self.prior_sd = prior_sd
        self.sparse_sd = sparse_sd
        
        self.gmm = GaussianMixtureModel(prior_sd, sparse_sd)
        
        self.encoder = EncoderNetwork(activation=activation)
        for p in self.parameters():
            p.data = torch.randn_like(p.data) * 0.001


    def forward(self, x):
        x = x @ self.w1.T + self.b1
        x = self.activation_fn(x)
        x = x @ self.w2.T + self.b2
        x = self.activation_fn(x)
        x = x @ self.w3.T + self.b3
        return x

    
    def encode_mean(self, latent):
        self.w1, self.b1, self.w2, self.b2, self.w3, self.b3 = self.encoder.forward_mean(latent)
        
    def mixture_gaussian_prior(self, sparsity=None):
        if sparsity is None:
            sparsity = self.sparsity
        log_prior = 0
        for p in self.parameters():
            log_prior += self.gmm.log_prob(p.flatten(), sparsity).sum()
        return log_prior
    
    def theta_loss(self, latent):
        self.w1, self.b1, self.w2, self.b2, self.w3, self.b3 = self.encoder.forward_mean(latent)
        return self.encoder.batch_loss(latent)
    
class EncoderNetwork(nn.Module):
    def __init__(self, input_dim=3, hidden_dim=286, activation=nn.ReLU):
        super(EncoderNetwork, self).__init__()
        # Define the initial layers
        self.fc1 = nn.Linear(input_dim, hidden_dim)  # Input layer
        self.fc2 = nn.Linear(hidden_dim, 15 * 1)     # Layer to produce [15, 1]
        self.fc3 = nn.Linear(hidden_dim, 15)         # Layer to produce [15]
        self.fc4 = nn.Linear(hidden_dim, 15 * 15)    # Layer to produce [15, 15]
        self.fc5 = nn.Linear(hidden_dim, 15)         # Layer to produce [15]
        self.fc6 = nn.Linear(hidden_dim, 1 * 15)     # Layer to produce [1, 15]
        self.fc7 = nn.Linear(hidden_dim, 1)          # Layer to produce [1]
        
        self.loss = nn.MSELoss(reduction='sum')
        self.N = hidden_dim
        self.activation_fn = activation()

    def forward_batch(self, x):
        # Pass through the first hidden layer
        x = self.activation_fn(self.fc1(x))  # [batch_size, hidden_dim]
        
        # Produce different outputs for each desired shape
        out1 = self.fc2(x).view(-1, 15, 1)  # Output shape: [15, 1]
        out2 = self.fc3(x).view(-1, 15)     # Output shape: [15]
        out3 = self.fc4(x).view(-1, 15, 15) # Output shape: [15, 15]
        out4 = self.fc5(x).view(-1, 15)     # Output shape: [15]
        out5 = self.fc6(x).view(-1, 1, 15)  # Output shape: [1, 15]
        out6 = self.fc7(x).view(-1, 1)      # Output shape: [1]

        return out1, out2, out3, out4, out5, out6
    
    def batch_loss(self, x):
        x = self.activation_fn(self.fc1(x))
        
        out1 = self.fc2(x).view(-1, 15, 1)  # Output shape: [15, 1]
        out2 = self.fc3(x).view(-1, 15)     # Output shape: [15]
        out3 = self.fc4(x).view(-1, 15, 15) # Output shape: [15, 15]
        out4 = self.fc5(x).view(-1, 15)     # Output shape: [15]
        out5 = self.fc6(x).view(-1, 1, 15)  # Output shape: [1, 15]
        out6 = self.fc7(x).view(-1, 1)   
        

        loss1 = self.loss(out1, out1.mean(dim=0).repeat(out1.shape[0], 1, 1))
        loss2 = self.loss(out2, out2.mean(dim=0).repeat(out2.shape[0], 1))
        loss3 = self.loss(out3, out3.mean(dim=0).repeat(out3.shape[0], 1, 1))
        loss4 = self.loss(out4, out4.mean(dim=0).repeat(out4.shape[0], 1))
        loss5 = self.loss(out5, out5.mean(dim=0).repeat(out5.shape[0], 1, 1))
        loss6 = self.loss(out6, out6.mean(dim=0).repeat(out6.shape[0], 1))

        sparse_loss = self.sparsify(out1.mean(dim=0)) + self.sparsify(out2) + self.sparsify(out3.mean(dim=0)) + self.sparsify(out4) + self.sparsify(out5.mean(dim=0)) + self.sparsify(out6)
        return (loss1 + loss2 + loss3 + loss4 + loss5 + loss6) / self.N + sparse_loss
    
    def sparsify(self, x):
        masked_loss = torch.where(x.abs() > 0.1, torch.zeros_like(x.abs()), x.abs())
        return masked_loss.sum()
    
    def forward_mean(self, x):
        x = self.activation_fn(self.fc1(x))  # [batch_size, hidden_dim]
        
        # Produce different outputs for each desired shape
        out1 = self.fc2(x).view(-1, 15, 1).mean(dim=0)  # Output shape: [15, 1]
        out2 = self.fc3(x).view(-1, 15).mean(dim=0)     # Output shape: [15]
        out3 = self.fc4(x).view(-1, 15, 15).mean(dim=0) # Output shape: [15, 15]
        out4 = self.fc5(x).view(-1, 15).mean(dim=0)     # Output shape: [15]
        out5 = self.fc6(x).view(-1, 1, 15).mean(dim=0)  # Output shape: [1, 15]
        out6 = self.fc7(x).view(-1, 1).mean(dim=0)      # Output shape: [1]

        return out1, out2, out3, out4, out5, out6
    
class GaussianMixtureModel:
    def __init__(self, prior_sd, sparse_sd):
        """
        Initialize the GMM with means, standard deviations, and mixture weights for each component.
        :param means: A tensor of shape (2,) containing the means of the two Gaussians.
        :param stds: A tensor of shape (2,) containing the standard deviations of the two Gaussians.
        :param mixture_weights: A tensor of shape (2,) containing the mixture weights of the two Gaussians.
        """
        self.means = torch.tensor([0.0, 0.0])  # Means of the two Gaussians
        self.stds = torch.tensor([prior_sd, sparse_sd])   # Standard deviations of the two Gaussians
        self.components = dist.Normal(self.means, self.stds)


    def log_prob(self, x, sparsity):
        """
        Calculate the log probability of data points x under the GMM.
        :param x: A tensor of data points.
        :return: A tensor of log probabilities.
        """
        # Calculate log probabilities from each component for each data point
        log_probs = self.components.log_prob(x.unsqueeze(1))  # Shape will be [N, num_components]
        # Weight log probabilities by the log of mixture weights
        self.mixture_weights = torch.tensor([sparsity, 1-sparsity])  # Ensure weights sum to 1
        log_weighted_probs = log_probs + torch.log(self.mixture_weights)
        # Log-sum-exp trick for numerical stability: log(sum(exp(log_probs)))
        log_sum_exp = torch.logsumexp(log_weighted_probs, dim=1)
        return log_sum_exp
    
if __name__ == "__main__":
    
    
    net = EFI_Net()
    x = torch.randn(5, 1)
    latent = torch.randn(10, 3)
    
    loss = net.theta_loss(latent)
    print(loss)
    pred = net(x)    
    print(pred)
    
    # optimizer.zero_grad()
    # loss = encoder.batch_loss(x)
    # loss.backward()
    # for p in encoder.parameters():
    #     print(p.grad)
    
    # print(out1.shape, out2.shape, out3.shape, out4.shape, out5.shape, out6.shape)

    # efi = EFI_Net()
    # x = torch.randn(10, 3)
    # efi.encode_mean(x)
    # print(efi.w1.shape, efi.b1.shape, efi.w2.shape, efi.b2.shape, efi.w3.shape, efi.b3.shape)
    
    # x = torch.randn(10, 1)
    # out = efi(x)
    # print(out)
    
    # for p in efi.parameters():
    #     print(p.shape)  
 
    
    # # Example usage
    # input_dim = 10
    # hidden_dim = 20
    # output_dim = 1

    # # Initialize NetG (which generates parameters) and NetF (which uses the parameters)
    # param_input_dim = 5  # Size of input to NetG
    # g = NetG(param_input_dim, input_dim, hidden_dim, output_dim)
    # f = NetF(input_dim, hidden_dim, output_dim)

    # # Example inputs
    # phi = torch.randn(1, param_input_dim)  # Input to NetG (parameter generator)
    # x = torch.randn(1, input_dim)  # Input to NetF (main network)

    # # Generate parameters for NetF using NetG
    # theta = g(phi)

    # # Forward pass through NetF using the generated parameters
    # output = f(x, theta)

    # print("Output of NetF:", output)
    
    
    # for p in f.parameters():
    #     print('netF', p.shape)
        
    # for p in g.parameters():
    #     print('netG', p.shape)